// @generated by build.rs. Do not edit.
use anyhow::{anyhow, Result};

use crate::graph::{AttrValue, OpAttrs};
use crate::ops::cpu::registry::expect_output;
use crate::tensor::{DType, TensorValue};

use super::kernels::{normal, packed};

pub fn cast_normal_dispatch(attrs: &OpAttrs, inputs: &[TensorValue], output: Option<&mut TensorValue>) -> Result<()> {
    let out = expect_output(output)?;
    let to_dtype = get_to_dtype(attrs)?;
    let in_dtype = inputs[0].dtype();
    if !is_allowed_cast(in_dtype, to_dtype) {
        return Err(anyhow!("unsupported cast from {:?} to {:?}", in_dtype, to_dtype));
    }

    match (&inputs[0], out) {
        (TensorValue::I8(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::i8_to_f64),
        (TensorValue::I8(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::i8_to_f64),
        (TensorValue::I16(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::i16_to_f64),
        (TensorValue::I16(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::i16_to_f64),
        (TensorValue::I32(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::i32_to_f64),
        (TensorValue::I32(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::i32_to_f64),
        (TensorValue::I32(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::i32_to_f64),
        (TensorValue::I32(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::i32_to_f64),
        (TensorValue::I32(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::i32_to_f64),
        (TensorValue::I32(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::i32_to_f64),
        (TensorValue::I64(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::i64_to_f64),
        (TensorValue::I64(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::i64_to_f64),
        (TensorValue::I64(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::i64_to_f64),
        (TensorValue::I64(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::i64_to_f64),
        (TensorValue::I64(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::i64_to_f64),
        (TensorValue::U8(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::u8_to_f64),
        (TensorValue::U8(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::u8_to_f64),
        (TensorValue::U16(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::u16_to_f64),
        (TensorValue::U16(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::u16_to_f64),
        (TensorValue::U32(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::u32_to_f64),
        (TensorValue::U32(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::u32_to_f64),
        (TensorValue::U32(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::u32_to_f64),
        (TensorValue::U32(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::u32_to_f64),
        (TensorValue::U32(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::u32_to_f64),
        (TensorValue::U32(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::u32_to_f64),
        (TensorValue::U64(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::u64_to_f64),
        (TensorValue::U64(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::u64_to_f64),
        (TensorValue::U64(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::u64_to_f64),
        (TensorValue::U64(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::u64_to_f64),
        (TensorValue::U64(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::u64_to_f64),
        (TensorValue::F8(a0), TensorValue::I8(out)) => normal::cast_to_i8(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::U8(out)) => normal::cast_to_u8(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::f8_to_f64),
        (TensorValue::F8(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::f8_to_f64),
        (TensorValue::F16(a0), TensorValue::I8(out)) => normal::cast_to_i8(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::U8(out)) => normal::cast_to_u8(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::f16_to_f64),
        (TensorValue::F16(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::f16_to_f64),
        (TensorValue::BF16(a0), TensorValue::I8(out)) => normal::cast_to_i8(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::U8(out)) => normal::cast_to_u8(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::bf16_to_f64),
        (TensorValue::BF16(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::bf16_to_f64),
        (TensorValue::F32(a0), TensorValue::I8(out)) => normal::cast_to_i8(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::U8(out)) => normal::cast_to_u8(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::f32_to_f64),
        (TensorValue::F32(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::f32_to_f64),
        (TensorValue::F64(a0), TensorValue::I8(out)) => normal::cast_to_i8(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::I16(out)) => normal::cast_to_i16(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::I32(out)) => normal::cast_to_i32(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::I64(out)) => normal::cast_to_i64(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::U8(out)) => normal::cast_to_u8(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::U16(out)) => normal::cast_to_u16(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::U32(out)) => normal::cast_to_u32(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::U64(out)) => normal::cast_to_u64(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::F8(out)) => normal::cast_to_f8(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::F16(out)) => normal::cast_to_f16(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::BF16(out)) => normal::cast_to_bf16(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::F32(out)) => normal::cast_to_f32(a0, out, normal::f64_to_f64),
        (TensorValue::F64(a0), TensorValue::F64(out)) => normal::cast_to_f64(a0, out, normal::f64_to_f64),
        (TensorValue::I1(a0), TensorValue::I8(out)) => packed::cast_packed_signed(a0, out, 1, |v| v),
        (TensorValue::I1(a0), TensorValue::I16(out)) => packed::cast_packed_signed(a0, out, 1, |v| v as i16),
        (TensorValue::I1(a0), TensorValue::I32(out)) => packed::cast_packed_signed(a0, out, 1, |v| v as i32),
        (TensorValue::I1(a0), TensorValue::I64(out)) => packed::cast_packed_signed(a0, out, 1, |v| v as i64),
        (TensorValue::I1(a0), TensorValue::F8(out)) => packed::cast_packed_signed(a0, out, 1, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::I1(a0), TensorValue::F16(out)) => packed::cast_packed_signed(a0, out, 1, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::I1(a0), TensorValue::BF16(out)) => packed::cast_packed_signed(a0, out, 1, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::I1(a0), TensorValue::F32(out)) => packed::cast_packed_signed(a0, out, 1, |v| v as f32),
        (TensorValue::I1(a0), TensorValue::F64(out)) => packed::cast_packed_signed(a0, out, 1, |v| v as f64),
        (TensorValue::I2(a0), TensorValue::I8(out)) => packed::cast_packed_signed(a0, out, 2, |v| v),
        (TensorValue::I2(a0), TensorValue::I16(out)) => packed::cast_packed_signed(a0, out, 2, |v| v as i16),
        (TensorValue::I2(a0), TensorValue::I32(out)) => packed::cast_packed_signed(a0, out, 2, |v| v as i32),
        (TensorValue::I2(a0), TensorValue::I64(out)) => packed::cast_packed_signed(a0, out, 2, |v| v as i64),
        (TensorValue::I2(a0), TensorValue::F8(out)) => packed::cast_packed_signed(a0, out, 2, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::I2(a0), TensorValue::F16(out)) => packed::cast_packed_signed(a0, out, 2, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::I2(a0), TensorValue::BF16(out)) => packed::cast_packed_signed(a0, out, 2, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::I2(a0), TensorValue::F32(out)) => packed::cast_packed_signed(a0, out, 2, |v| v as f32),
        (TensorValue::I2(a0), TensorValue::F64(out)) => packed::cast_packed_signed(a0, out, 2, |v| v as f64),
        (TensorValue::I4(a0), TensorValue::I8(out)) => packed::cast_packed_signed(a0, out, 4, |v| v),
        (TensorValue::I4(a0), TensorValue::I16(out)) => packed::cast_packed_signed(a0, out, 4, |v| v as i16),
        (TensorValue::I4(a0), TensorValue::I32(out)) => packed::cast_packed_signed(a0, out, 4, |v| v as i32),
        (TensorValue::I4(a0), TensorValue::I64(out)) => packed::cast_packed_signed(a0, out, 4, |v| v as i64),
        (TensorValue::I4(a0), TensorValue::F8(out)) => packed::cast_packed_signed(a0, out, 4, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::I4(a0), TensorValue::F16(out)) => packed::cast_packed_signed(a0, out, 4, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::I4(a0), TensorValue::BF16(out)) => packed::cast_packed_signed(a0, out, 4, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::I4(a0), TensorValue::F32(out)) => packed::cast_packed_signed(a0, out, 4, |v| v as f32),
        (TensorValue::I4(a0), TensorValue::F64(out)) => packed::cast_packed_signed(a0, out, 4, |v| v as f64),
        (TensorValue::U1(a0), TensorValue::U8(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v),
        (TensorValue::U1(a0), TensorValue::U16(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v as u16),
        (TensorValue::U1(a0), TensorValue::U32(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v as u32),
        (TensorValue::U1(a0), TensorValue::U64(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v as u64),
        (TensorValue::U1(a0), TensorValue::F8(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::U1(a0), TensorValue::F16(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::U1(a0), TensorValue::BF16(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::U1(a0), TensorValue::F32(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v as f32),
        (TensorValue::U1(a0), TensorValue::F64(out)) => packed::cast_packed_unsigned(a0, out, 1, |v| v as f64),
        (TensorValue::U2(a0), TensorValue::U8(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v),
        (TensorValue::U2(a0), TensorValue::U16(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v as u16),
        (TensorValue::U2(a0), TensorValue::U32(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v as u32),
        (TensorValue::U2(a0), TensorValue::U64(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v as u64),
        (TensorValue::U2(a0), TensorValue::F8(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::U2(a0), TensorValue::F16(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::U2(a0), TensorValue::BF16(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::U2(a0), TensorValue::F32(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v as f32),
        (TensorValue::U2(a0), TensorValue::F64(out)) => packed::cast_packed_unsigned(a0, out, 2, |v| v as f64),
        (TensorValue::U4(a0), TensorValue::U8(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v),
        (TensorValue::U4(a0), TensorValue::U16(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v as u16),
        (TensorValue::U4(a0), TensorValue::U32(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v as u32),
        (TensorValue::U4(a0), TensorValue::U64(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v as u64),
        (TensorValue::U4(a0), TensorValue::F8(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| normal::f64_to_f8(v as f64)),
        (TensorValue::U4(a0), TensorValue::F16(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| normal::f64_to_f16(v as f64)),
        (TensorValue::U4(a0), TensorValue::BF16(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| normal::f64_to_bf16(v as f64)),
        (TensorValue::U4(a0), TensorValue::F32(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v as f32),
        (TensorValue::U4(a0), TensorValue::F64(out)) => packed::cast_packed_unsigned(a0, out, 4, |v| v as f64),
        _ => Err(anyhow!("dtype mismatch")),
    }
}

fn get_to_dtype(attrs: &OpAttrs) -> Result<DType> {
    attrs
        .items
        .iter()
        .find(|attr| attr.name == "to")
        .ok_or_else(|| anyhow!("missing to attribute"))
        .and_then(|attr| match &attr.value {
            AttrValue::DType(dtype) => Ok(*dtype),
            _ => Err(anyhow!("to attribute must be a dtype")),
        })
}

pub fn is_allowed_cast(in_dtype: DType, out_dtype: DType) -> bool {
    if is_float(out_dtype) {
        return is_packed_signed(in_dtype)
            || is_packed_unsigned(in_dtype)
            || is_signed_int(in_dtype)
            || is_unsigned_int(in_dtype)
            || is_float(in_dtype);
    }

    if is_signed_int(out_dtype) {
        if is_float(in_dtype) {
            return true;
        }
        if is_packed_signed(in_dtype) {
            return out_dtype.bit_width() > in_dtype.bit_width();
        }
        if is_signed_int(in_dtype) {
            return out_dtype.bit_width() > in_dtype.bit_width();
        }
        return false;
    }

    if is_unsigned_int(out_dtype) {
        if is_float(in_dtype) {
            return true;
        }
        if is_packed_unsigned(in_dtype) {
            return out_dtype.bit_width() > in_dtype.bit_width();
        }
        if is_unsigned_int(in_dtype) {
            return out_dtype.bit_width() > in_dtype.bit_width();
        }
        return false;
    }

    false
}

fn is_float(dtype: DType) -> bool {
    matches!(dtype, DType::F8 | DType::F16 | DType::BF16 | DType::F32 | DType::F64)
}

fn is_signed_int(dtype: DType) -> bool {
    matches!(dtype, DType::I8 | DType::I16 | DType::I32 | DType::I64)
}

fn is_unsigned_int(dtype: DType) -> bool {
    matches!(dtype, DType::U8 | DType::U16 | DType::U32 | DType::U64)
}

fn is_packed_signed(dtype: DType) -> bool {
    matches!(dtype, DType::I1 | DType::I2 | DType::I4)
}

fn is_packed_unsigned(dtype: DType) -> bool {
    matches!(dtype, DType::U1 | DType::U2 | DType::U4)
}
